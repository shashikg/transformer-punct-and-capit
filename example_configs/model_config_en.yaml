experiment_details:
  save_dir: experiments
  model_name: bert_uncased_L-12_H-768_A-12-EN
model:
  pretrained_model_name: google/bert_uncased_L-12_H-768_A-12
  freeze_pretrained_model: false
  use_crf: true
  use_bilstm: false
  labels_dict:
    O|O: 0
    C|O: 1
    U|O: 2
    O|.: 3
    C|.: 4
    U|.: 5
    O|,: 6
    C|,: 7
    U|,: 8
    O|?: 9
    C|?: 10
    U|?: 11
  labels_order: c|p
  classification_head:
    intermediate_layers: null
    dropout: 0.1
    activation: ReLU
tokenizer:
  path: experiments/tokenizers/bert_uncased_L-12_H-768_A-12
  train_new: false
  use_pretrained_tokenizer: true
  pretrained_model_name: google/bert_uncased_L-12_H-768_A-12
dataset:
  data_dir: data/train_data
  processed_data_dir: data/preprocessed_text_data
  max_seq_length: 64
  labels_order: c|p
  punct_labels: O|.|,|?
  capit_labels: O|C|U
  train_ds:
    parsed_data_file: data/train_data/train_64.msgpack
    json_data_file: train.json
    datasets: tatoeba
    per_data_max_sentence: 3000000
    shuffle: true
    batch_size: 256
    num_workers: 16
  dev_ds:
    parsed_data_file: data/train_data/dev_64.msgpack
    json_data_file: dev.json
    datasets: tatoeba
    per_data_max_sentence: 3000
    shuffle: false
    batch_size: 256
    num_workers: 16
  augmentation:
    rate: 0.15
    type: all
    sub_style: unk
    alpha_sub: 0.4
    alpha_del: 0.4
  sent_augmentation:
    combine_ratio: 0.66
    cut_ratio: 0.2
pl_trainer:
  max_epochs: 20
  accumulate_grad_batches: 1
  checkpoint_callback: true
  logger: true
  log_every_n_steps: 1
  check_val_every_n_epoch: 1
optim:
  lr: 0.0001
  weight_decay: 1.0e-06
